#!/usr/bin/env python
"""
Write changepoint and classification information into JSON files.
"""

import argparse
import math
import numpy
import os
import os.path
import rpy2
import rpy2.interactive.packages
import rpy2.robjects
import sys

from rpy2.rinterface import R_VERSION_BUILD

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import read_krun_results_file, write_krun_results_file


class Segment(object):
    """A single segment between two changepoints.
    """

    def __init__(self, delta, start, end, mean, variance, data, outliers):
        self.delta = delta
        self.start = start
        self.end = end
        self.mean = mean
        self.variance = variance
        self.data = data
        # Remove outliers before calculating confidence interval.
        segment_data = data[:]
        for outlier in sorted(outliers, reverse=True):
            del segment_data[outlier]
        self.outliers = outliers
        # Use a robust method (inter-quartile range) to calculate a confidence
        # interval. We use IQR because it is robust to outliers and non-normal
        # distributions of data, and in practice we found it generates slightly
        # thinner than the Tukey interval used by mark_outliers_in_json. This
        # helps prevent specious merging of segments which can lead to a
        # misclassification.
        q1, q3 = numpy.percentile(segment_data, 25.0), numpy.percentile(segment_data, 75.0)
        self.iqr = q3 - q1
        self.lower_ci = q1 - 1.5 * self.iqr
        self.upper_ci = q3 + 1.5 * self.iqr

    def overlaps_with(self, other):
        """Return True if the confidence interval of 'other' segment overlaps
        with this segment, and False otherwise.
        """
        assert isinstance(other, Segment)
        if ((self.upper_ci < other.lower_ci or other.upper_ci < self.lower_ci)
            and math.fabs(self.mean - other.mean) > self.delta):
            return False
        return True

    @property
    def n(self):
        return self.end - self.start


class Segments(object):
    """A list of Segments for a whole run sequence.
    """

    def __init__(self, delta, half_bound, steady_state, length, cpts, means,
                 variances, data, outliers):
        self.delta = delta
        self.half_bound = half_bound
        self.steady_state = steady_state
        self.length = length  # Length of original data with outliers.
        assert self.length == len(data)
        self.data = data
        self.outliers = outliers
        self.segments = list()
        assert len(means) == len(variances) == len(cpts)
        if len(means) == 1:  # No changepoints.
            segment = Segment(delta, 0, self.length - 1, means[0], variances[0], data,
                              outliers)
            self.segments.append(segment)
        else:
            for index in xrange(len(means)):
                segment = None
                if index == 0:
                    s_out = [out for out in outliers if out <= cpts[index]]
                    segment = Segment(delta, 0, cpts[index], means[index],
                                  variances[index], data[:cpts[index]+1], s_out)
                else:
                    s_out = list()
                    for out in outliers:
                         if out > (cpts[index - 1]) and out <= cpts[index]:
                             s_out.append(out - cpts[index - 1] - 1)
                    segment = Segment(delta, cpts[index - 1], cpts[index],
                                  means[index], variances[index],
                                  data[cpts[index - 1]+1:cpts[index]+1], s_out)
                self.segments.append(segment)
        assert cpts[:-1] == [s.end for s in self.segments][:-1]

    @property
    def means(self):
        return [segment.mean for segment in self.segments]

    @property
    def changepoints(self):
        """Return all changepoints.
        The last location in the data is always a changepoint, so we ignore it.
        """
        if len(self.segments) == 1:
            return list()
        return [segment.end for segment in self.segments][:-1]

    def _pooled_means(self, mu0, n0, mu1, n1):
        return (mu0 * n0 + mu1 * n1) / (n0+n1)

    def _pooled_variances(self, var0, mu0, n0, var1, mu1, n1):
        mu01_sq = math.pow(self._pooled_means(mu0, n0, mu1, n1), 2)
        mu0_sq = math.pow(mu0, 2)
        mu1_sq = math.pow(mu1, 2)
        return (((n0 * (var0 + mu0_sq) + n1 * (var1 + mu1_sq)) / (n0 + n1)) -
                 mu01_sq)

    def _merge_segment(self, index):
        """Merge an individual segment with its neighbour."""
        first = self.segments[index]
        second = self.segments[index + 1]
        mean = self._pooled_means(first.mean, first.n, second.mean, second.n)
        var_ = self._pooled_variances(first.variance, first.mean, first.n,
                          second.variance, second.mean, second.n)
        # Merge segment 1 and 2
        merged = Segment(self.delta, first.start, second.end, mean, var_,
                     first.data + second.data, first.outliers + second.outliers)
        # Replace segment 1 with the merged segment.
        self.segments[index] = merged
        # Delete segment 2.
        del self.segments[index + 1]

    def merge_overlapping_segments(self):
        """Merge adjacent segments whose confidence intervals overlap.

        We do not consider the first segment initially because in a classic
        warm-up execution, the first segment will be short, and will a much
        larger mean than other segments, and therefore any measure of variation
        will be very large . If we include the first segment in the
        overlaps_with_last_segment calculations, many warm-up executions will be
        classified as flat. Instead we consider the first segment separately,
        and use an absolute measure of difference between it and any subsequent
        segment means.
        """
        if len(self.segments) > 2:
            i = 1
            to_merge = list()
            while i < len(self.segments) - 1:
                orig_i = i
                for j in range(orig_i + 1, len(self.segments)):
                    no_overlap = False
                    for k in range(orig_i, j):
                        if not self.segments[k].overlaps_with(self.segments[j]):
                            no_overlap = True
                            break
                    i += 1
                    if no_overlap:
                        break
                    to_merge.append(j - 1)
            if to_merge:
                for index in sorted(to_merge, reverse=True):
                    self._merge_segment(index)
        # In a classic "warmup", the first segment may have only a small number
        # of data (possibly one datum). So a confidence interval in this case
        # will be wide, and will overlap with all later segments, leaving a flat
        # classification. Instead, of using a confidence interval, we consider
        # the absolute difference between the means of first two segments.
        if len(self.segments) >= 2:
            if math.fabs(self.segments[0].mean - self.segments[1].mean) < self.delta:
                self._merge_segment(0)

    def get_classification(self):
        """Return a classification for this run sequence."""

        upper_bound = 1.0 + self.half_bound
        lower_bound = 1.0 - self.half_bound
        classification = 'flat'
        last_segment = self.segments[-1]  # Hopefully the steady state segment.
        for index in xrange(len(self.segments) - 2, -1, -1):
            current_segment = self.segments[index]
            if (current_segment.mean > last_segment.mean * lower_bound and
                current_segment.mean < last_segment.mean * upper_bound):
                continue
            elif current_segment.end > (self.length - self.steady_state):
                classification = 'no steady state'
                break
            elif current_segment.mean < last_segment.mean * lower_bound:
                classification = 'slowdown'
                break
            assert current_segment.mean > last_segment.mean * upper_bound
            classification = 'warmup'
        return classification


def main(in_files, delta, half_bound, steady_state):
    cpt = rpy2.interactive.packages.importr('changepoint')
    r_version = '.'.join(R_VERSION_BUILD[:2])
    print 'Using R version %s and changepoint library %s' % (r_version, cpt.__version__)
    assert cpt.__version__ >= '2.2.2', 'Please update the changepoint library.'
    assert r_version >= '3.3.1', 'Please update R from CRAN.'
    krun_data = dict()
    for filename in in_files:
        assert os.path.exists(filename), 'File %s does not exist.' % filename
        print 'Loading: %s' % filename
        krun_data[filename] = read_krun_results_file(filename)
    for filename in krun_data:
        changepoints = dict()
        classifications = dict()
        changepoint_means = dict()
        rm_outliers = 'all_outliers' in krun_data[filename]
        if not rm_outliers:
            print ('No all_outliers key in %s; please run '
                   './bin/mark_outliers_in_json on your data if you want this '
                   'analysis to exclude outliers.'% filename)
        for bench in sorted(krun_data[filename]['wallclock_times']):
            changepoints[bench] = list()
            classifications[bench] = list()
            changepoint_means[bench] = list()
            for index, p_exec in enumerate(krun_data[filename]['wallclock_times'][bench]):
                if rm_outliers:
                    outliers = krun_data[filename]['all_outliers'][bench][index]
                else:
                    outliers = list()
                segments = get_segments(cpt, delta, half_bound, steady_state,
                                        p_exec, outliers)
                if delta > 0.0:
                    segments.merge_overlapping_segments()
                changepoints[bench].append(segments.changepoints)
                changepoint_means[bench].append(segments.means)
                try:
                    classifications[bench].append(segments.get_classification())
                except ValueError:
                    print 'Could not classify %s execution %d' % (bench, index + 1)
                    sys.exit(1)
        krun_data[filename]['changepoints'] = changepoints
        krun_data[filename]['changepoint_means'] = changepoint_means
        krun_data[filename]['classifications'] = classifications
        krun_data[filename]['steady_state_expected'] = steady_state
        new_filename = create_output_filename(filename)
        print 'Writing out: %s' % new_filename
        write_krun_results_file(krun_data[filename], new_filename)


def get_segments(cpt, delta, half_bound, steady_state, data, outliers):
    p_exec = data[:]  # data will be passed to Segments unchanged.
    length = len(p_exec)  # Will change when we remove outliers.
    indices = sorted(outliers, reverse=True)
    for index in indices:
        del p_exec[index]
    measurements = rpy2.robjects.FloatVector(p_exec)
    changepoints = cpt.cpt_meanvar(measurements, method='PELT', penalty='Manual',
                                   pen_value=15.0*numpy.log(len(p_exec)))
    # List indices in R start at 1.
    c_points = [int(cpoint - 1) for cpoint in changepoints.slots['cpts']]
    # If outliers were deleted, the index of each changepoint will have moved.
    # Here, we adjust the indices to match the original data.
    for outlier in outliers:
        for index in xrange(len(c_points)):
            if c_points[index] >= outlier:
                c_points[index] += 1
    # Variances is a list of variances for each data segment between changepoints.
    means, variances = list(), list()
    for mean in changepoints.slots['param.est'][changepoints.slots['param.est'].names.index('mean')]:
        means.append(float(mean))
    for var_ in changepoints.slots['param.est'][changepoints.slots['param.est'].names.index('variance')]:
        variances.append(float(var_))
    return Segments(delta, half_bound, steady_state, length, c_points, means, variances, data, outliers)


def create_output_filename(in_file_name):
    directory = os.path.dirname(in_file_name)
    basename = os.path.basename(in_file_name)
    if basename.endswith('.json.bz2'):
        root_name = basename[:-9]
    else:
        root_name = os.path.splitext(basename)[0]
    base_out = root_name + '_changepoints.json.bz2'
    return os.path.join(directory, base_out)


def create_cli_parser():
    """Create a parser to deal with command line switches.
    """
    script = os.path.basename(__file__)
    description = ("""Write changepoints and classifications into Krun results
file(s). If you want outliers to be excluded from the changepoint calculations,
you should first run your data through the ./bin/mark_outliers_in_json script.

This script does not alter your original Krun results file. Instead it writes
out a new file, with _changepoints added to the filename. For example if the
input file is:

    results_outliers_w200.json.bz2

the output of this script will be a new file named:

    results_outliers_w200_changepoints.json.bz2.

Example usage:
    $ python %s results1.json.bz2
    $ python %s  --steady 500 --delta 0.001 --bound 0.001 results1.json.bz2 results2.json.bz2\n""" % (script, script))
    parser = argparse.ArgumentParser(description)
    parser.add_argument('json_files', nargs='+', action='append', default=[],
                        type=str, help='One or more Krun result files.')
    parser.add_argument('--steady', '-s', action='store', dest='steady_state',
                        default=500, type=int, metavar='N',
                        help=('Expect a steady state should be reached before '
                              'the last N iterations.'))
    parser.add_argument('--delta', '-d', action='store', dest='delta',
                        default=0.001, type=float, metavar='D',
                        help=('Adjacent segments means must differ by D or be '
                              'merged. If D <= 0 then no attempt will be made to '
                              'merge adjacent segments (this is likely to be '
                              'useful only in testing).'))
    parser.add_argument('--bound', '-b', action='store', dest='half_bound',
                        default=0.001, type=float, metavar='B',
                        help=('Adjacent segments must differ by more than B%% in '
                              'order to be considered a warmup or slowdown.'))
    return parser


if __name__ == '__main__':
    parser = create_cli_parser()
    options = parser.parse_args()
    print ('Marking changepoints and classifications, expecting a steady state '
           'to be reached before the last %d iterations.' %
           options.steady_state)
    main(options.json_files[0], options.delta, options.half_bound, options.steady_state)
