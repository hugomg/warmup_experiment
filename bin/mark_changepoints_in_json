#!/usr/bin/env python
"""
Write changepoint and classification information into JSON files.
"""

import argparse
import math
import numpy
import os
import os.path
import rpy2
import rpy2.interactive.packages
import rpy2.robjects
import sys

from rpy2.rinterface import R_VERSION_BUILD

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import read_krun_results_file, write_krun_results_file


# In several places we need to determine whether the absolute difference between
# two segment means is over a given threshold. ABSOLUTE_DELTA is that threshold.
ABSOLUTE_DELTA = 0.001


class Segment(object):
    """A single segment between two changepoints.
    """

    def __init__(self, start, end, mean, variance, data, outliers):
        self.start = start
        self.end = end
        self.mean = mean
        self.variance = variance
        self.data = data
        segment_data = data[:]
        for outlier in sorted(outliers, reverse=True):
            del segment_data[outlier]
        self.outliers = outliers
        self.median = numpy.median(segment_data)
        self.ci = 3.0 * (numpy.percentile(segment_data, 90.0) - numpy.percentile(segment_data, 10.0))
        self.lower_ci = self.median - self.ci
        self.upper_ci = self.median + self.ci

    def overlaps_with(self, other):
        """Return True if the confidence interval of 'other' segment overlaps
        with this segment.
        """
        assert isinstance(other, Segment)
        if self.upper_ci < other.lower_ci or other.upper_ci < self.lower_ci:
            return False
        return True

    @property
    def n(self):
        return self.end - self.start

class Segments(object):
    """A list of Segments for a whole run sequence.
    """

    def __init__(self, length, steady_state, cpts, means, variances, data, outliers):
        self.length = length  # Length of original data with outliers.
        assert self.length == len(data)
        self.data = data
        self.outliers = outliers
        self.steady_state = steady_state
        self.segments = list()
        assert len(means) == len(variances) == len(cpts)
        if len(means) == 1:  # No changepoints.
            segment = Segment(0, self.length - 1, means[0], variances[0], data,
                              outliers)
            self.segments.append(segment)
        else:
            for index in xrange(len(means)):
                segment = None
                if index == 0:
                    s_out = [out for out in outliers if out <= cpts[index]]
                    segment = Segment(0, cpts[index], means[index],
                                  variances[index], data[:cpts[index]+1], s_out)
                else:
                    s_out = list()
                    for out in outliers:
                         if out > (cpts[index - 1]) and out <= cpts[index]:
                             s_out.append(out - cpts[index - 1] - 1)
                    segment = Segment(cpts[index - 1], cpts[index], means[index],
                                  variances[index],
                                  data[cpts[index - 1]+1:cpts[index]+1], s_out)
                self.segments.append(segment)
        assert cpts[:-1] == [s.end for s in self.segments][:-1]

    @property
    def means(self):
        return [segment.mean for segment in self.segments]

    @property
    def changepoints(self):
        """Return all changepoints.
        The last location in the data is always a changepoint, so we ignore it.
        """
        if len(self.segments) == 1:
            return list()
        return [segment.end for segment in self.segments][:-1]

    def _pooled_means(self, mu0, n0, mu1, n1):
        return (mu0 * n0 + mu1 * n1) / (n0+n1)

    def _pooled_variances(self, var0, mu0, n0, var1, mu1, n1):
        mu01_sq = math.pow(self._pooled_means(mu0, n0, mu1, n1), 2)
        mu0_sq = math.pow(mu0, 2)
        mu1_sq = math.pow(mu1, 2)
        return (((n0 * (var0 + mu0_sq) + n1 * (var1 + mu1_sq)) / (n0 + n1)) -
                 mu01_sq)

    def _merge_segment(self, index):
        """Merge an individual segment with its neighbour."""
        first = self.segments[index]
        second = self.segments[index + 1]
        mean = self._pooled_means(first.mean, first.n, second.mean, second.n)
        var_ = self._pooled_variances(first.variance, first.mean, first.n,
                          second.variance, second.mean, second.n)
        # Merge segment 1 and 2
        merged = Segment(first.start, second.end, mean, var_,
                     first.data + second.data, first.outliers + second.outliers)
        # Replace segment 1 with the merged segment.
        self.segments[index] = merged
        # Delete segment 2.
        del self.segments[index + 1]

    def merge_overlapping_segments(self):
        """Merge adjacent segments whose confidence intervals overlap.
        """
        # Merge overlapping segments, ignoring the first segment.
        while True:
            for index in xrange(1, len(self.segments) - 1):
                if self.segments[index].overlaps_with(self.segments[index + 1]):
                    self._merge_segment(index)
                    break
            else:  # for-loop reached last segment, break out of while.
                break
        # In a classic "warmup", the first segment may have only a small number
        # of data (possibly one datum). So a confidence interval in this case
        # will be wide, and will overlap with all later segments, leaving a flat
        # classification. Instead, we consider the absolute difference between
        # the means of first two segments.
        if len(self.segments) >= 2:
            if math.fabs(self.segments[0].mean - self.segments[1].mean) < ABSOLUTE_DELTA:
                self._merge_segment(0)

    def get_classification(self):
        """Return the classification for this run sequence.
        """
        classification = ''
        # We do not use the first segment in the overlaps_with_last_segment
        # calculations. This is because in a classic warm-up execution, the
        # first segment will be short, and will a much larger mean than other
        # segments, and therefore any measure of variation will be very large .
        # If we include the first segment in the overlaps_with_last_segment
        # calculations, every warm-up execution will be classified as flat.
        overlaps_with_last_segment = list()
        for segment in self.segments[1:-1]:
            overlaps_with_last_segment.append(segment.overlaps_with(self.segments[-1]))
        # Flat (a) -> no changepoints.
        if len(self.segments) == 1:
            classification = 'flat'
        # Slowdown -> last mean+ci does not overlap with another mean+ci and
        #             it is higher than the first mean+ci.
        elif self.segments[-1].mean == max(self.segments, key=lambda s: s.mean).mean:
            classification = 'slowdown'
        # Warmup -> first segment mean is highest, and other segments overlap.
        #          OR first segment mean is highest, and last is the lowest.
        elif (self.segments[0].mean == max(self.segments, key=lambda s: s.mean).mean
              and self.segments[0].mean - self.segments[1].mean > ABSOLUTE_DELTA
              and (all(overlaps_with_last_segment) or
                   self.segments[-1].mean == min(self.segments, key=lambda s: s.mean).mean)):
            classification = 'warmup'
        # Flat (b) -> 2 or more segments, whose confidence intervals overlap.
        elif (all(overlaps_with_last_segment) and
              self.segments[0].mean - self.segments[1].mean <= ABSOLUTE_DELTA):
            classification = 'flat'
        # Slow down -> changes larger than the confidence interval after the
        #              first segment and no changepoints after we expect a
        #              steady state.
        elif self.segments[-1].start <= (self.length - self.steady_state):
            classification = 'slowdown'
        # No steady state -> there are changes after we expect steady state.
        elif self.segments[-1].start > (self.length - self.steady_state):
            classification = 'no steady state'
        else:
            raise ValueError('Could not classify data.')
        return classification


def main(in_files, steady_state, penalty=False):
    cpt = rpy2.interactive.packages.importr('changepoint')
    r_version = '.'.join(R_VERSION_BUILD[:2])
    print 'Using R version %s and changepoint library %s' % (r_version, cpt.__version__)
    assert cpt.__version__ >= '2.2.2', 'Please update the changepoint library.'
    assert r_version >= '3.3.1', 'Please update R from CRAN.'
    krun_data = dict()
    for filename in in_files:
        assert os.path.exists(filename), 'File %s does not exist.' % filename
        print 'Loading: %s' % filename
        krun_data[filename] = read_krun_results_file(filename)
    for filename in krun_data:
        changepoints = dict()
        classifications = dict()
        changepoint_means = dict()
        rm_outliers = 'all_outliers' in krun_data[filename]
        if not rm_outliers:
            print ('No all_outliers key in %s; please run '
                   './bin/mark_outliers_in_json on your data if you want this '
                   'analysis to exclude outliers.'% filename)
        for bench in sorted(krun_data[filename]['wallclock_times']):
            changepoints[bench] = list()
            classifications[bench] = list()
            changepoint_means[bench] = list()
            for index, p_exec in enumerate(krun_data[filename]['wallclock_times'][bench]):
                if rm_outliers:
                    outliers = krun_data[filename]['all_outliers'][bench][index]
                else:
                    outliers = list()
                segments = get_segments(cpt, p_exec, steady_state, outliers,
                                        use_snr_penalty=penalty)
                segments.merge_overlapping_segments()
                changepoints[bench].append(segments.changepoints)
                changepoint_means[bench].append(segments.means)
                try:
                    classifications[bench].append(segments.get_classification())
                except ValueError:
                    print 'Could not classify %s execution %d' % (bench, index + 1)
                    sys.exit(1)
        krun_data[filename]['changepoints'] = changepoints
        krun_data[filename]['changepoint_means'] = changepoint_means
        krun_data[filename]['classifications'] = classifications
        krun_data[filename]['steady_state_expected'] = steady_state
        new_filename = create_output_filename(filename)
        print 'Writing out: %s' % new_filename
        write_krun_results_file(krun_data[filename], new_filename)


def _get_snr_penalty(data):
    penalty = numpy.mean(data) / numpy.std(data)
    if numpy.isnan(penalty) or numpy.isinf(penalty):
        raise ValueError()
    else:
        return penalty


def get_segments(cpt, data, steady_state, outliers, use_snr_penalty=False):
    p_exec = data[:]  # data will be passed to Segments unchanged.
    length = len(p_exec)  # Will change when we remove outliers.
    indices = sorted(outliers, reverse=True)
    for index in indices:
        del p_exec[index]
    measurements = rpy2.robjects.FloatVector(p_exec)
    if use_snr_penalty:  # User has asked for SNR penalty.
        try:  # Check it doesn't return NaN or Inf.
            penalty = _get_snr_penalty(p_exec[-steady_state:])
            changepoints = cpt.cpt_meanvar(measurements, method='PELT',
                                           penalty='Manual', pen_value=penalty)
        except ValueError:  # Use default penalty, 'MBIC'.
            changepoints = cpt.cpt_meanvar(measurements, method='PELT')
    else:
        changepoints = cpt.cpt_meanvar(measurements, method='PELT', penalty='Manual',
                                       pen_value=15.0*numpy.log(len(p_exec)))
    # List indices in R start at 1.
    c_points = [int(cpoint - 1) for cpoint in changepoints.slots['cpts']]
    # If outliers were deleted, the index of each changepoint will have moved.
    # Here, we adjust the indices to match the original data.
    for outlier in outliers:
        for index in xrange(len(c_points)):
            if c_points[index] >= outlier:
                c_points[index] += 1
    # Variances is a list of variances for each data segment between changepoints.
    means, variances = list(), list()
    for mean in changepoints.slots['param.est'][changepoints.slots['param.est'].names.index('mean')]:
        means.append(float(mean))
    for var_ in changepoints.slots['param.est'][changepoints.slots['param.est'].names.index('variance')]:
        variances.append(float(var_))
    return Segments(length, steady_state, c_points, means, variances, data, outliers)


def create_output_filename(in_file_name):
    directory = os.path.dirname(in_file_name)
    basename = os.path.basename(in_file_name)
    if basename.endswith('.json.bz2'):
        root_name = basename[:-9]
    else:
        root_name = os.path.splitext(basename)[0]
    base_out = root_name + '_changepoints.json.bz2'
    return os.path.join(directory, base_out)


def create_cli_parser():
    """Create a parser to deal with command line switches.
    """
    script = os.path.basename(__file__)
    description = ("""Write changepoints and classifications into Krun results
file(s). If you want outliers to be excluded from the changepoint calculations,
you should first run your data through the ./bin/mark_outliers_in_json script.

This script does not alter your original Krun results file. Instead it writes
out a new file, with _changepoints added to the filename. For example if the
input file is:

    results_outliers_w200.json.bz2

the output of this script will be a new file named:

    results_outliers_w200_changepoints.json.bz2.

Example usage:
    $ python %s results1.json.bz2
    $ python %s  --steady 500 -p results1.json.bz2 results2.json.bz2\n""" % (script, script))
    parser = argparse.ArgumentParser(description)
    parser.add_argument('json_files', nargs='+', action='append', default=[],
                        type=str, help='One or more Krun result files.')
    parser.add_argument('--steady', '-s', action='store', dest='steady_state',
                        default=200, type=int, metavar='N',
                        help=('Expact a steady state should be reached before '
                              'the last N iterations.'))
    parser.add_argument('--penalty', '-p', action='store_true', dest='penalty',
                        default=False,
                        help=('Use mean/std as a penalty in changepoint detection. '
                              'This option should be used if you have noisy data '
                              '(e.g. not from the Krun framework) and you find '
                              'the default settings produce a large, specious '
                              'number of changepoints.'))
    return parser


if __name__ == '__main__':
    parser = create_cli_parser()
    options = parser.parse_args()
    print ('Marking changepoints and classifications, expecting a steady state '
           'to be reached before the last %d iterations.' %
           options.steady_state)
    main(options.json_files[0], options.steady_state, options.penalty)
