#!/usr/bin/env python2.7
"""
Plot data from Krun results file(s).
"""

import argparse
import datetime
import math
import matplotlib
matplotlib.use('Agg')
import numpy
import numpy.random
import os
import os.path
import seaborn
import sys

from matplotlib import gridspec, pyplot
from matplotlib.collections import LineCollection

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import pretty_print_machine, read_krun_results_file
from warmup.outliers import get_window
from warmup.plotting import add_margin_to_axes, compute_grid_offsets
from warmup.plotting import format_yticks_scientific, get_unified_yrange, style_axis
from warmup.vm_instruments import INSTRUMENTATION_PARSERS

seaborn.set_palette('colorblind')
seaborn.set_style('whitegrid', {'lines.linewidth': 1.0, 'axes.linewidth': 1.0})
seaborn.set_context('paper')

pyplot.figure(tight_layout=True)

# Configures border and spacing of subplots.
# Here we just make it more space efficient for the paper
SUBPLOT_PARAMS = {
    'hspace': 0.35,
    'bottom': 0.07,
    'left': 0.07,
    'right': 0.93,
    'top': 0.90,
    'wspace': 0.30,
}

# Display names that can't be formatted with .title()
BENCHMARKS = {
    'binarytrees':      'Binary Trees',
    'spectralnorm':     'Spectral Norm',
    'fannkuch_redux':   'Fannkuch Redux',
    'nbody':            'N-Body',
}

GRID_MINOR_X_DIVS = 20
GRID_MAJOR_X_DIVS = 10

GRID_MINOR_Y_DIVS = 12
GRID_MAJOR_Y_DIVS = 6

GRID_MINOR_Y_DIVS_SMALLER_PLOTS = 4
GRID_MAJOR_Y_DIVS_SMALLER_PLOTS = 2

LINE_COLOUR = 'k'
FILL_ALPHA = 0.2

# line widths, measured in pts
LINE_WIDTH = 1
PLOT_LINE_WIDTH = 0.4

OUTLIER_MARKER = 'o'
OUTLIER_COLOR = 'r'
OUTLIER_SIZE = 80
UNIQUE_COLOR = 'g'
UNIQUE_MARKER = 'o'
UNIQUE_SIZE = 80
COMMON_COLOR = 'r'
COMMON_MARKER = '*'
COMMON_SIZE = 80
CYCLES_COLOR = 'black'
PERF_COLOR = 'black'
INSTR_COLOR = 'black'

CHANGEPOINT_LINE_STYLE = '-'
CHANGEPOINT_LINE_COLOR = 'r'

ZORDER_GRID = 1
ZORDER_DATA = 5
ZORDER_FILL_REGION = 7
ZORDER_MEDIAN = 8
ZORDER_CHANGEPOINTS = 10
ZORDER_MARKERS = 20

# Gap between right-hand y-axes.
TWINX_OFFSET = 1.1

# Use a fixed y-range for the aperf/mperf data.
PERF_YRANGE = (0.0, 1.2)
PERF_YTICK_FORMAT = '%.1f'

# Inset placement (left, bottom, width, height) relative to subplot axis.
INSET_DEFAULT_WIDTH = 0.4
INSET_DEFAULT_HEIGHT = 0.25
INSET_PADDING = 0.035
INSET_TICK_FONTSIZE = 8

# Default (PDF) font sizes
TICK_FONTSIZE = 8
TITLE_FONT_SIZE = 10
AXIS_FONTSIZE = 8
BASE_FONTSIZE = 8
LEGEND_FONTSIZE = 10
YTICK_FORMAT = '%.4f'


FONT = {
    'family': 'sans',
    'weight': 'regular',
    'size': BASE_FONTSIZE,
}
matplotlib.rc('font', **FONT)

MAX_SUBPLOTS_PER_ROW = 2
EXPORT_SIZE_INCHES = [12, 10]
DPI = 300


def get_instr_data(key, machine, instr_dir, pexec_idxs):
    """Get the instrumentation data summary for the specified process execution
    indexes"""

    bench, vm, variant = key.split(":")
    ret = []
    if vm in INSTRUMENTATION_PARSERS:
        for pexec_idx in pexec_idxs:
            file_ = os.path.join(instr_dir, "%s__%s__%s__%s.json.bz2" %
                                 (bench, vm, variant, pexec_idx))
            print("Loading: %s" % file_)
            try:
                js = read_krun_results_file(file_)
            except IOError:
                print("WARNING: Missing instrumentation data for: %s:%s:%s" % \
                      (machine, key, pexec_idx))
                ret.append(None)  # missing instr data
                continue
            parser = INSTRUMENTATION_PARSERS[vm](js)
            ret.append(parser.chart_data)
            del js  # This can be huge, so eagerly GC it
        return ret


def main(is_interactive, data_dcts, plot_titles, window_size, outfile,
         xlimits, with_outliers, unique_outliers, changepoints, changepoint_means,
         median=False, tukey=False, inset=False, one_page=False,
         legend_off=True, core_cycles=(0,1,2,3), cycles_ylimits=None):
    """Determine which plots to put on each page of output.
    Plot all data.
    """

    pdf = None  # PDF output (for non-interactive mode).

    if not is_interactive:
        pdf = PdfPages(outfile)
        set_pdf_metadata(pdf)

    # Run sequences, outliers and subplot titles for each page we need to plot.
    pages, all_subplot_titles = list(), list()
    cycles_pages = list()
    instr_pages = list()
    all_outliers, all_common, all_unique = list(), list(), list()
    all_changepoints, all_changepoint_means = list(), list()

    # By default, each benchmark from each machine is placed on a separate
    # page. If the --one-page switch has been passed in, then we place
    # all plots on a single page. In which case, we need to construct
    # a flat list of all run sequences.
    if one_page:
        page, subplot_titles = list(), list()
        cycles_page = list()
        instr_page = list()
        page_common, page_unique, page_outlier = list(), list(), list()
        page_changepoints, page_changepoint_means = list(), list()
        for key in sorted(data_dcts['data']):
            for machine in sorted(data_dcts['data'][key]):
                for index, run_seq in enumerate(data_dcts['data'][key][machine]):
                    page.append(run_seq)
                    if data_dcts['cycles_counts'][key][machine]:
                        cycles_page.append(data_dcts['cycles_counts'][key][machine][index])
                    if data_dcts['instr_data'][key][machine]:
                        instr_page.append(data_dcts['instr_data'][key][machine][index])
                    subplot_titles.append(plot_titles[key][machine][index])
                    # Collect outliers, if the user wishes to annotate them on
                    # the plots. The draw_page() and draw_subplot() functions
                    # expect either lists of outliers or None (if outliers are
                    # not to be annotated).
                    if with_outliers:
                        page_outlier.append(data_dcts['all_outliers'][key][machine][index])
                    else:
                        page_outlier.append(None)
                    if unique_outliers:
                        page_common.append(data_dcts['common_outliers'][key][machine][index])
                        page_unique.append(data_dcts['unique_outliers'][key][machine][index])
                    else:
                        page_common.append(None)
                        page_unique.append(None)
                    if changepoints:
                        page_changepoints.append(data_dcts['changepoints'][key][machine][index])
                        page_changepoint_means.append(None)
                    if changepoint_means:
                        page_changepoints.append(data_dcts['changepoints'][key][machine][index])
                        page_changepoint_means.append(data_dcts['changepoint_means'][key][machine][index])
                    else:
                        page_changepoints.append(None)
                        page_changepoint_means.append(None)
        pages.append(page)
        cycles_pages.append(cycles_page)
        instr_pages.append(instr_page)
        all_subplot_titles.append(subplot_titles)
        all_outliers.append(page_outlier)
        all_common.append(page_common)
        all_unique.append(page_unique)
        all_changepoints.append(page_changepoints)
        all_changepoint_means.append(page_changepoint_means)
    else:  # Create multiple pages.
        for key in sorted(data_dcts['data']):
            for machine in sorted(data_dcts['data'][key]):
                pages.append(data_dcts['data'][key][machine])
                cycles_pages.append(data_dcts['cycles_counts'][key][machine])
                instr_pages.append(data_dcts['instr_data'][key][machine])
                all_subplot_titles.append(plot_titles[key][machine])
                if with_outliers:
                    all_outliers.append(data_dcts['all_outliers'][key][machine])
                else:
                    all_outliers.append(None)
                if unique_outliers:
                    all_common.append(data_dcts['common_outliers'][key][machine])
                    all_unique.append(data_dcts['unique_outliers'][key][machine])
                else:
                    all_common.append(None)
                    all_unique.append(None)
                if changepoints:
                    all_changepoints.append(data_dcts['changepoints'][key][machine])
                    all_changepoint_means.append(None)
                elif changepoint_means:
                    all_changepoints.append(data_dcts['changepoints'][key][machine])
                    all_changepoint_means.append(data_dcts['changepoint_means'][key][machine])
                else:
                    all_changepoints.append(None)
                    all_changepoint_means.append(None)

    # Draw each page and display (interactive mode) or save to disk.
    try:
        for index, page in enumerate(pages):
            bmark, vm, mc = all_subplot_titles[index][0].split(', ')[:3]
            print 'Plotting %s: %s (%s) on page %02d of %02d.' % \
                  (mc, bmark, vm, index + 1, len(pages))

            # Strip out indices where the benchmark crashed.
            def only_uncrashed(data):
                if data is None:
                    return None
                ret = list()
                for i in xrange(len(page)):
                    if page[i]:
                        ret.append(data[i])
                    else:
                        if data == page:  # Stops repeated printing of warning.
                            print("WARNING: requested pexec crashed: "
                                  "%s, %s, %s, %s" % (mc, bmark, vm, index + 1))
                return ret

            wct_page = only_uncrashed(page)
            cycles_page = only_uncrashed(cycles_pages[index])
            instr_page = only_uncrashed(instr_pages[index])
            subplot_titles = only_uncrashed(all_subplot_titles[index])
            outliers = only_uncrashed(all_outliers[index])
            common = only_uncrashed(all_common[index])
            unique = only_uncrashed(all_unique[index])
            changepoints = only_uncrashed(all_changepoints[index])
            changepoint_means = only_uncrashed(all_changepoint_means[index])

            rv = draw_page(is_interactive, wct_page, cycles_page,
                                         instr_page, subplot_titles,
                                         window_size, xlimits, outliers,
                                         unique, common, changepoints,
                                         changepoint_means, median, tukey,
                                         inset, legend_off, core_cycles,
                                         cycles_ylimits)
            if rv is not None:
                fig, export_size = rv
                if not is_interactive:
                    fig.set_size_inches(*export_size)
                    pdf.savefig(fig, dpi=fig.dpi, orientation='landscape',
                                bbox_inches='tight')
                    pyplot.close()
    except KeyboardInterrupt:
        pass  # Avoid printing a traceback.
    finally:
        if not is_interactive:
            pdf.close()
            print('Saved: %s' % outfile)


def add_inset_to_axis(fig, axis, rect):
    """Adds a new axis to an existing axis, located at rect.
    rect should be a 4-tuple of (left, bottom, width, height) all relative
    to the axis.
    """
    left, bottom, width, height = rect
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    fig_left, fig_bottom = transform((left, bottom))
    fig_width, fig_height = transform([width, height]) - transform([0, 0])
    return fig.add_axes([fig_left, fig_bottom, fig_width, fig_height])


def _get_scatter_points_within_bounds(scatter, x_bounds):
    """Given a set of x-locations to be plotted as a scatter plot, move the
    marker locations to within x_bounds. This is needed when we have a set
    of outliers to plot, but we only want to view a small section of the
    whole chart.
    """
    x_vals = [val for val in scatter if val >= x_bounds[0] and val < x_bounds[1]]
    y_vals = [val - x_bounds[0] for val in scatter if val >= x_bounds[0] and val < x_bounds[1]]
    return x_vals, y_vals


def draw_process_exec(grid_cell, data, cycles_data,
                 instr_data, instr_y_ranges, title, x_bounds, y_range, window_size,
                 outliers, unique, common, changepoints, changepoint_means,
                 median, tukey, core_cycles, cycles_ylimits):
    iterations = numpy.array(xrange(x_bounds[0], x_bounds[1]))
    # Marshal data into numpy arrays.
    data_narray = numpy.array(data[x_bounds[0]:x_bounds[1]])
    cycle_data_narrays = list()
    if cycles_data:
        if cycles_ylimits:
            cycles_min, cycles_max = cycles_ylimits
        else:
            if core_cycles is None:
                core_cycles = range(len(cycles_data))
            cycles_min, cycles_max = float('inf'), float('-inf')
            for core in core_cycles:
                cycles_min = min(min(cycles_data[core][x_bounds[0]:x_bounds[1]]), cycles_min)
                cycles_max = max(max(cycles_data[core][x_bounds[0]:x_bounds[1]]), cycles_max)
        for core in core_cycles:  # For each core specified on command line.
            cycle_data_narrays.append(numpy.array(cycles_data[core][x_bounds[0]:x_bounds[1]]))
    instr_data_narrays = list()
    if instr_data:
        for chart_data in instr_data:
            instr_data_narrays.append(numpy.array(chart_data.data))
    # Calculate the number of plots needed.
    n_rows = 1
    if cycles_data:
        n_rows += len(core_cycles)
    if instr_data:
        n_rows += len(instr_data_narrays)
    if n_rows == 1:  # Only create another gridspec if we have to.
        inner_grid = None
    else:
        # Spring constraints for grid cell heights
        heights = [.25] * (n_rows - 1)
        heights.append(1)
        inner_grid = gridspec.GridSpecFromSubplotSpec(n_rows, 1, grid_cell,
                                                      height_ratios=heights,
                                                      hspace=0.2)
    # Collect handles and labels, so that we can draw one legend per page.
    handles_labels = [list(), list()]
    # Plot wall clock times.
    if n_rows == 1:
        axis1 = pyplot.subplot(grid_cell)
    else:
        axis1 = pyplot.subplot(inner_grid[n_rows - 1, 0])
    axis1.plot(iterations, data_narray, label='Measurement', color=LINE_COLOUR,
               zorder=ZORDER_DATA, linewidth=PLOT_LINE_WIDTH)
    # If there are outliers, highlight them.
    axis1.autoscale(enable=False, axis='x') # Avoid scatter() changing xlimits.
    if outliers is not None:
        outliers_x, outliers_y = _get_scatter_points_within_bounds(outliers, x_bounds)
        axis1.scatter(outliers_x, data_narray[outliers_y], color=OUTLIER_COLOR,
                     marker=OUTLIER_MARKER, s=OUTLIER_SIZE, label='Outliers', zorder=ZORDER_MARKERS)
    if unique is not None:
        unique_x, unique_y = _get_scatter_points_within_bounds(unique, x_bounds)
        pc_unique = float(len(unique_y) - window_size) / float(len(data_narray)) * 100.0
        axis1.scatter(unique_x, data_narray[unique_y], c=UNIQUE_COLOR,
                     marker=UNIQUE_MARKER, s=UNIQUE_SIZE,
                     label=('Unique outliers (${%.2f}\\%%$)' % pc_unique),
                     zorder=ZORDER_MARKERS)
    if common is not None:
        common_x, common_y = _get_scatter_points_within_bounds(common, x_bounds)
        pc_common = float(len(common_y)) / float(len(data_narray)) * 100.0
        axis1.scatter(common_x, data_narray[common_y], c=COMMON_COLOR,
                     marker=COMMON_MARKER, s=COMMON_SIZE,
                     label=('Common outliers (${%.2f}\\%%$)' % pc_common),
                     zorder=ZORDER_MARKERS)
    # Draw change points and means between them, optionally.
    if changepoint_means:
        means = list()  # Draw means between ((x0, y0), (x1, y1)) pairs.
        assert len(changepoints) == len(changepoint_means) - 1
        for index, x_location in enumerate(changepoints):
            # Draw changepoint.
            axis1.axvline(x_location, linestyle=CHANGEPOINT_LINE_STYLE,
                         zorder=ZORDER_CHANGEPOINTS, color=CHANGEPOINT_LINE_COLOR,
                         linewidth=LINE_WIDTH)
            if index == 0:
                means.append([(0, changepoint_means[0]),
                             (x_location, changepoint_means[0])])
            else:
                means.append([(changepoints[index - 1], changepoint_means[index]),
                             (x_location, changepoint_means[index])])
        if len(changepoints) == 0:
            # No changepoints in this execution, but we want to draw the mean.
            means.append([(0, changepoint_means[-1]),
                         (len(data_narray), changepoint_means[-1])])
        else:
            means.append([(changepoints[-1], changepoint_means[-1]),
                         (len(data_narray), changepoint_means[-1])])
        # Add changepoint means to chart, on top of all other splines.
        assert len(means) == len(changepoint_means)
        lines = LineCollection(means, color=CHANGEPOINT_LINE_COLOR,
                               zorder=ZORDER_CHANGEPOINTS,
                               linewidths=[LINE_WIDTH for _ in xrange(len(means))])
        axis1.add_collection(lines)
    # Draw changepoints as blobs, without changepoint means.
    if changepoints and not changepoint_means:
        axis1.scatter(changepoints, [y_range[0] for _ in changepoints],
                      c=LINE_COLOUR, marker=OUTLIER_MARKER, s=OUTLIER_SIZE,
                      zorder=ZORDER_CHANGEPOINTS)
    # Draw a rolling median (optionally).
    if median or tukey:
        medians = list()
        pc_bands = (list(), list())
        for index, datum in enumerate(data[x_bounds[0]:x_bounds[1]]):
            window = get_window(index, window_size, data[x_bounds[0]:x_bounds[1]])
            if not window:
                medians.append(numpy.nan)
            else:
                medians.append(numpy.median(window))
            if tukey and not window:
                pc_bands[0].append(numpy.nan)
                pc_bands[1].append(numpy.nan)
            elif tukey and window:
                band = (3 * (numpy.percentile(window, 90.0) -
                             numpy.percentile(window, 10.0)))
                pc_bands[0].append(medians[index] - band)
                pc_bands[1].append(medians[index] + band)
        if median:  # Plot the median.
            axis1.plot(iterations, medians, label='Median',
                       zorder=ZORDER_MEDIAN, linewidth=PLOT_LINE_WIDTH)
        if tukey:  # Fill between the Tukey band.
            axis1.fill_between(iterations, pc_bands[0], pc_bands[1], alpha=FILL_ALPHA,
                               facecolor=LINE_COLOUR, edgecolor=LINE_COLOUR,
                               zorder=ZORDER_FILL_REGION)
    # Re-style the chart.
    major_xticks = compute_grid_offsets(
        x_bounds[0], x_bounds[1], GRID_MAJOR_X_DIVS)
    minor_xticks = compute_grid_offsets(
        x_bounds[0],x_bounds[1], GRID_MINOR_X_DIVS)
    major_yticks = compute_grid_offsets(
        y_range[0], y_range[1], GRID_MAJOR_Y_DIVS)
    minor_yticks = compute_grid_offsets(
        y_range[0], y_range[1], GRID_MINOR_Y_DIVS)
    style_axis(axis1, major_xticks, minor_xticks, major_yticks, minor_yticks, TICK_FONTSIZE)
    format_yticks_scientific(axis1)
    # Plot title goes above the top subplot.
    if not cycles_data:
        axis1.set_title(title, fontsize=TITLE_FONT_SIZE)
    # Axis labels.
    axis1.set_xlabel('In-process iteration', fontsize=AXIS_FONTSIZE)
    axis1.set_ylabel('Time (secs)', fontsize=AXIS_FONTSIZE)
    axis1.yaxis.set_label_position('right')
    # Add all items to page legend.
    handles, labels = axis1.get_legend_handles_labels()
    handles_labels[0] += handles
    handles_labels[1] += labels
    # Other charts.
    row = 0
    # Add core cycles data on another set of charts.
    if cycles_data:
        for core in xrange(len(cycle_data_narrays)):
            axis = pyplot.subplot(inner_grid[row, 0], sharex=axis1)
            pyplot.setp(axis.get_xticklabels(), visible=False)
            axis.set_ylim(cycles_min, cycles_max)
            axis.plot(iterations, cycle_data_narrays[core],
                      color=CYCLES_COLOR, label=('Core %d cycles' % core_cycles[core]),
                      linewidth=PLOT_LINE_WIDTH, zorder=ZORDER_DATA)
            # Style axes and grid.
            y_ax = axis.get_yaxis()
            y_ax.set_tick_params(labelsize=TICK_FONTSIZE, colors=CYCLES_COLOR, zorder=ZORDER_GRID)
            axis.set_ylabel('CC #%d' % core_cycles[core], fontsize=AXIS_FONTSIZE, color=CYCLES_COLOR)
            axis.yaxis.set_label_position('right')
            ymin, ymax = axis.get_ylim()
            major_yticks = compute_grid_offsets(ymin, ymax, GRID_MAJOR_Y_DIVS_SMALLER_PLOTS)
            minor_yticks = compute_grid_offsets(ymin, ymax, GRID_MINOR_Y_DIVS_SMALLER_PLOTS)
            style_axis(axis, major_xticks, minor_xticks, major_yticks, minor_yticks, TICK_FONTSIZE)
            format_yticks_scientific(axis)
            if core == 0:
                # Add all items to page legend (only once).
                handles, labels = axis.get_legend_handles_labels()
                handles_labels[0] += handles
                handles_labels[1].append('Normalised core cycles')
                # Plot title goes above the top subplot (only once).
                axis.set_title(title, fontsize=TITLE_FONT_SIZE)
            row += 1
    # Add any VM instrumentation data on another set of charts.
    if instr_data:
        legend_texts = set()
        for index in xrange(len(instr_data_narrays)):
            legend_texts.add(instr_data[index].legend_text)
        for index in xrange(len(instr_data_narrays)):
            axis = pyplot.subplot(inner_grid[row, 0], sharex=axis1)
            pyplot.setp(axis.get_xticklabels(), visible=False)
            axis.plot(iterations, instr_data_narrays[index],
                        color=INSTR_COLOR, label=(instr_data[index].title),
                      linewidth=PLOT_LINE_WIDTH, zorder=ZORDER_DATA)
            y_ax = axis.get_yaxis()
            y_ax.set_tick_params(labelsize=TICK_FONTSIZE, colors=INSTR_COLOR)
            axis.set_ylabel(instr_data[index].title, fontsize=AXIS_FONTSIZE, color=INSTR_COLOR)
            axis.yaxis.set_label_position('right')
            axis.set_ylim(instr_y_ranges[index])
            major_yticks = compute_grid_offsets(instr_y_ranges[index][0],
                                   instr_y_ranges[index][1], GRID_MAJOR_Y_DIVS_SMALLER_PLOTS)
            minor_yticks = compute_grid_offsets(instr_y_ranges[index][0],
                                   instr_y_ranges[index][1], GRID_MINOR_Y_DIVS_SMALLER_PLOTS)
            style_axis(axis, major_xticks, minor_xticks, major_yticks, minor_yticks, TICK_FONTSIZE)
            format_yticks_scientific(axis)
            if index == 0:
                # Add all items to page legend (only once).
                handles, labels = axis.get_legend_handles_labels()
                handles_labels[0] += handles
                handles_labels[1].append(', '.join(legend_texts))
                # Plot title goes above the top subplot (only once).
                if not cycles_data:
                    axis.set_title(title, fontsize=TITLE_FONT_SIZE)
            row += 1
    axis1.set_ylim(y_range)
    # Margin MUST be adjusted after plotting is complete.
    add_margin_to_axes(axis1, x=0.02, y=0.02)
    # Return lines and labels so that we can draw a single legend for the page.
    return axis1, handles_labels


def draw_page(is_interactive, executions, cycles_executions,
              instr_executions, titles, window_size, xlimits,
              outliers, unique, common, changepoints, changepoint_means, median,
              tukey, inset=False, legend_off=True, core_cycles=(0,1,2,3),
              cycles_ylimits=None):
    """Plot a page of benchmarks.
    """

    n_execs = len(executions)
    if n_execs == 0:
        print("WARNING: empty page")
        return None

    n_rows = int(math.ceil(float(len(executions)) / MAX_SUBPLOTS_PER_ROW))
    n_cols = min(MAX_SUBPLOTS_PER_ROW, n_execs)

    print('%g plots arranged in %g rows and %g columns.'
          % (len(executions), n_rows, n_cols))

    if xlimits is None:
        xlimits_start = 0
        xlimits_stop = len(executions[0])  # Assume all execs are the same length
    else:
        try:
            xlimits_start, xlimits_stop = [int(x) for x in xlimits.split(',')]
        except ValueError:
            fatal_error('Invalid xlimits pair: %s' % xlimits)

    # Find the min and max y values across all wallclock time plots for this page.
    y_min, y_max = get_unified_yrange(executions, xlimits_start, xlimits_stop, padding=0.02)

    # Get unified y-ranges for the instrumentation data. Each VM may have more
    # more than one set of instrumentation data (e.g. GC events, JIT
    # compilation, etc.) and each VM instrument must have its own y-range.
    if instr_executions:
        instr_data_by_instrument = []
        for execution in instr_executions:
            if execution is None:
                continue  # absent instr data for this pexec
            instr_data_by_instrument = [list() for _ in execution]
            break
        for execution in instr_executions:
            if execution is None:
                for index in xrange(len(instr_data_by_instrument)):
                    instr_data_by_instrument[index].append(None)
            else:
                for index, instrument_data in enumerate(execution):
                    instr_data_by_instrument[index].append(instrument_data.data)
        instr_y_ranges = list()
        for chart_data in instr_data_by_instrument:
            chart_data_stripped = [x for x in chart_data if x is not None]
            instr_y_ranges.append(get_unified_yrange(chart_data_stripped, xlimits_start,
                                                     xlimits_stop, padding=0.02))
    else:
        instr_y_ranges = None

    fig = pyplot.figure()
    outer_grid = gridspec.GridSpec(n_rows, n_cols)
    # Keep a list of all the axes that wallclock times are drawn on, in case
    # we later need to draw an inset onto each one.
    wallclock_axes = list()

    index, row, col = 0, 0, 0
    cycles_data, instr_data = None, None
    outliers_exec, unique_exec, common_exec = None, None, None
    changepoint_exec, changepoint_mean_exec = None, None
    while index < n_execs:
        data = executions[index]
        if cycles_executions:
            cycles_data = cycles_executions[index]
        if instr_executions:
            instr_data = instr_executions[index]
        if outliers:
            outliers_exec = outliers[index]
        if unique:
            unique_exec = unique[index]
        if common:
            common_exec = common[index]
        if changepoints:
            changepoint_exec = changepoints[index]
        if changepoint_means:
            changepoint_mean_exec = changepoint_means[index]
        # Get axis and draw plot.
        inner_grid = outer_grid[row, col]
        x_bounds = [xlimits_start, xlimits_stop]
        wc_axis, (handles, labels) = draw_process_exec(inner_grid, data, cycles_data,
                              instr_data, instr_y_ranges,
                              titles[index], x_bounds, [y_min, y_max], window_size,
                              outliers_exec, unique_exec, common_exec, changepoint_exec,
                              changepoint_mean_exec, median, tukey, core_cycles,
                              cycles_ylimits)
        wallclock_axes.append(wc_axis)
        col += 1
        if col == MAX_SUBPLOTS_PER_ROW:
            col = 0
            row += 1
        index = row * MAX_SUBPLOTS_PER_ROW + col

    outer_grid.update(**SUBPLOT_PARAMS)

    # Draw an inset, if required. This MUST be done after adjusting subplots,
    # so that we can calculate the correct bounding box for the inset.
    if inset:
        index, row, col = 0, 0, 0
        while index < n_execs:
            axis = wallclock_axes[index]
            inset_rect = (1.0 - INSET_DEFAULT_WIDTH - INSET_PADDING,
                          1.0 - INSET_DEFAULT_HEIGHT - INSET_PADDING,
                          INSET_DEFAULT_WIDTH, INSET_DEFAULT_HEIGHT)
            inset = add_inset_to_axis(fig, axis, inset_rect)
            inset.set_ylim([y_min, y_max])  # Same scale as larger subplot.
            inset.grid(False)  # Too many lines and y-ticks looks very messy.
            inset.set_yticks([y_min, y_min + ((y_max - y_min) / 2.0), y_max])
            inset.yaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter(YTICK_FORMAT))
            inset.xaxis.set_tick_params(labelsize=INSET_TICK_FONTSIZE)
            inset.yaxis.set_tick_params(labelsize=INSET_TICK_FONTSIZE)
            # Which iterations go in the inset? By default the first 2.5%.
            one_pc = len(executions[index][x_bounds[0]:x_bounds[1]]) / 100.0  # 1% of the iterations.
            inset_xlimit = (0, int(one_pc * 2.5))
            # If all the changepoints occur at the start of the process exec,
            # include them all and a few extra executions.
            if changepoints and changepoints[index]:
                if changepoints[index][x_bounds[0]:x_bounds[1]][-1] <= int(one_pc * 5.0):
                    inset_xlimit = (0, (changepoints[index][x_bounds[0]:x_bounds[1]][-1]
                                        + int(one_pc * 2.0)))
            # Clamp the extent of the inset.
            if inset_xlimit[1] > 150:
                inset_xlimit = (0, 150)
            inset.plot(range(*inset_xlimit),  # Plot subset of the data.
                       executions[index][inset_xlimit[0]:inset_xlimit[1]],
                       color=LINE_COLOUR, linewidth=PLOT_LINE_WIDTH,
                       zorder=ZORDER_DATA)
            col += 1
            if col == MAX_SUBPLOTS_PER_ROW:
                col = 0
                row += 1
            index = row * MAX_SUBPLOTS_PER_ROW + col

    if tukey:  # Add Tukey band to legend.
        fill_patch = matplotlib.patches.Patch(color=LINE_COLOUR,
                                              alpha=FILL_ALPHA)
        handles.append(fill_patch)
        labels.append('$\pm3(90^\mathrm{th}-10^\mathrm{th} \mathrm{percentile})$')

    # One legend per page.
    if not legend_off:
        fig.legend(handles, labels, loc='upper center', fontsize=LEGEND_FONTSIZE,
                   ncol=10, borderaxespad=0)

    if is_interactive:
        mng = pyplot.get_current_fig_manager()
        mng.resize(*mng.window.maxsize())
        pyplot.show()
        pyplot.close()
        return None, None
    else:
        # Return the figure to be saved in a multipage PDF.
        # Caller MUST close pyplot.
        export_size = (EXPORT_SIZE_INCHES[0] * n_cols,
                       EXPORT_SIZE_INCHES[1] * n_rows)
        return fig, export_size


def set_pdf_metadata(pdf_document):
    """Set metadata fields inside a PDF document.
    """
    info_dict = pdf_document.infodict()
    info_dict['Title'] = 'Krun results'
    info_dict['Author'] = 'soft-dev.org'
    info_dict['Creator'] = 'http://github.com/softdevteam/warmup_experiment'
    info_dict['Subject'] = 'Benchmarking results'
    info_dict['Keywords']= ('benchmark experiment interpreter measurement ' +
                            'software virtual machine')
    info_dict['CreationDate'] = datetime.datetime.today()
    info_dict['ModDate'] = datetime.datetime.today()


def get_data_dictionaries(json_files, benchmarks=[], wallclock_only=False,
                          outliers=False, unique_outliers=False, changepoints=False):
    """Read a list of BZipped JSON files and return their contents as a
    dictionaries of key -> machine name -> results.

    This function returns ONLY the data that the user has requested on the
    command line. Therefore, we check carefully that all data can be found
    in the available Krun results files. Also, we pass back the title text
    for each subplot.
    """

    data_dictionary = {'data': dict(),  'cycles_counts': dict(),
                       'instr_data': dict(),  # Per-VM information.
                       'changepoints': dict(), 'changepoint_means': dict(),
                       'all_outliers': dict(), 'common_outliers': dict(),
                       'unique_outliers': dict(),
                      }

    plot_titles = dict()  # All subplot titles.

    # Find out what data the user requested.
    # In bmark key -> machine  -> process execs format.
    requested_data = dict()
    if benchmarks != []:
        for quintuplet in benchmarks:
            try:
                machine, bmark, vm, variant, pexec = quintuplet.split(':')
            except ValueError:
                fatal_error('Malformed benchmark: %s. The correct '
                            'format is: machine:benchmark:vm:variant:pexec '
                            'e.g. mc.example.com:fasta:PyPy:default-python:0')
            key = ':'.join([bmark, vm, variant])
            if key not in requested_data:
                requested_data[key] = dict()
            if machine not in requested_data[key]:
                requested_data[key][machine] = list()
            requested_data[key][machine].append(int(pexec))

    # Collect the requested data from Krun results files.
    for filename in json_files:
        if not os.path.exists(filename):
            fatal_error('File %s does not exist.' % filename)
        print('Loading: %s' % filename)

        # All benchmarking data from one Krun results file.
        data = read_krun_results_file(filename)

        # Check that data requested on the command line exists in the JSON.
        if not wallclock_only and not ('core_cycle_counts' in data):
                fatal_error('Core cycle counts not stored in %s. '
                            'Consider running this script with --wallclock-only.'
                            % filename)
        if unique_outliers or outliers:
            if not ('common_outliers' in data and 'unique_outliers' in data and
                    'all_outliers' in data):
                fatal_error('You requested that outliers be annotated '
                            'on your plots, but file %s does not'
                            'contain the relevant keys. Please run the '
                            'mark_outliers_in_json.py script before '
                            'proceeding.' % filename)
        if changepoints:
            if 'changepoints' not in data:
                fatal_error('You requested that changepoints be annotated '
                            'on your plots, but file %s does not'
                            'contain the relevant keys. Please run the '
                            'mark_changepoints_in_json.py script before '
                            'proceeding.' % filename)

        # Get machine name from Krun results file.
        machine = data['audit']['uname'].split(' ')[1]
        if '.' in machine:  # Strip domain names.
            machine = machine.split('.')[0]
        machine_name = pretty_print_machine(machine)

        # Is there instrumentation data with this results file?
        instr_data = False
        filename_root = filename[:-len('_results.json.bz2')]
        instr_dir = os.path.dirname(filename) + filename_root + '_instr_data'
        data_dictionary['instr_data'] = dict()
        if os.path.isdir(instr_dir):
            print 'Collecting instrumentation data for %s from %s.' % \
                (filename, instr_dir)
            instr_data = True
        else:
            print 'No VM instrumentation data is available.'

        # Collect any results requested from this file.
        if benchmarks == []:  # Chart all available data from this file.
            for key in data['wallclock_times']:
                bmark = key.split(':')[0]
                if len(data['wallclock_times'][key]) == 0:
                    print('WARNING: Skipping: %s from %s (no executions)' %
                          (key, machine))
                else:
                    if key not in data_dictionary['data']:
                        data_dictionary['data'][key] = dict()
                        data_dictionary['cycles_counts'][key] = dict()
                        data_dictionary['instr_data'][key] = dict()
                        data_dictionary['changepoints'][key] = dict()
                        data_dictionary['changepoint_means'][key] = dict()
                        data_dictionary['all_outliers'][key] = dict()
                        data_dictionary['common_outliers'][key] = dict()
                        data_dictionary['unique_outliers'][key] = dict()
                    data_dictionary['data'][key][machine] = data['wallclock_times'][key]
                    if wallclock_only:
                        data_dictionary['cycles_counts'][key][machine] = None
                        data_dictionary['instr_data'][key][machine] = None
                    else:
                        data_dictionary['cycles_counts'][key][machine] = data['core_cycle_counts'][key]
                        if instr_data:
                            data_dictionary['instr_data'][key][machine] =  \
                                get_instr_data(
                                    key, machine, instr_dir,
                                    xrange(len(data['wallclock_times'][key])))
                        else:
                            data_dictionary['instr_data'][key][machine] = None
                    if changepoints:
                        data_dictionary['changepoints'][key][machine] = data['changepoints'][key]
                        data_dictionary['changepoint_means'][key][machine] = data['changepoint_means'][key]
                    else:
                        data_dictionary['changepoints'][key][machine] = None
                        data_dictionary['changepoint_means'][key][machine] = None
                    if outliers or unique_outliers:
                        data_dictionary['all_outliers'][key][machine] = data['all_outliers'][key]
                        data_dictionary['common_outliers'][key][machine] = data['common_outliers'][key]
                        data_dictionary['unique_outliers'][key][machine] = data['unique_outliers'][key]
                    else:
                        data_dictionary['all_outliers'][key][machine] = None
                        data_dictionary['common_outliers'][key][machine] = None
                        data_dictionary['unique_outliers'][key][machine] = None
            # Construct plot titles for all data in this file.
            for key in data_dictionary['data']:
                if key not in plot_titles:
                    plot_titles[key] = dict()
                if machine not in plot_titles[key]:
                    plot_titles[key][machine] = list()
                benchmark_name = key.split(':')[0]
                if benchmark_name in BENCHMARKS:
                    benchmark_name = BENCHMARKS[benchmark_name]
                else:
                    benchmark_name = benchmark_name.title()

                # Add one title for each process execution
                try:
                    num_p_execs = len(data_dictionary['data'][key][machine])
                except KeyError:
                    pass  # no data for this
                else:
                    for p_exec in xrange(num_p_execs):
                        if changepoints:
                            classification = ' (%s)' % \
                                data['classifications'][key][p_exec]
                        else:
                            classification = ''
                        title = '%s, %s, %s, Proc. exec. #%d%s' % \
                                (benchmark_name,
                                 key.split(':')[1],
                                 machine_name,
                                 p_exec + 1,
                                 classification)
                        plot_titles[key][machine].append(title)
        else:  # Chart only the data specified on command line.
            for key in requested_data:
                if machine not in requested_data[key]:
                    continue
                if key not in data['wallclock_times']:
                    # Hope the key appears in another file, checked below.
                    continue

                # Scaffold entries if not existing
                if key not in data_dictionary['data']:
                    data_dictionary['data'][key] = dict()
                    data_dictionary['cycles_counts'][key] = dict()
                    data_dictionary['instr_data'][key] = dict()
                    data_dictionary['changepoints'][key] = dict()
                    data_dictionary['changepoint_means'][key] = dict()
                    data_dictionary['all_outliers'][key] = dict()
                    data_dictionary['common_outliers'][key] = dict()
                    data_dictionary['unique_outliers'][key] = dict()

                if machine not in data_dictionary['data'][key]:
                    data_dictionary['data'][key][machine] = list()
                    if not wallclock_only:
                        data_dictionary['cycles_counts'][key][machine] = list()
                        data_dictionary['instr_data'][key][machine] = list()
                    else:
                        data_dictionary['cycles_counts'][key][machine] = None
                        data_dictionary['instr_data'][key][machine] = None
                    if changepoints:
                        data_dictionary['changepoints'][key][machine] = list()
                        data_dictionary['changepoint_means'][key][machine] = list()
                    else:
                        data_dictionary['changepoints'][key][machine] = None
                        data_dictionary['changepoint_means'][key][machine] = None
                    if outliers or unique_outliers:
                        data_dictionary['all_outliers'][key][machine] = list()
                        data_dictionary['common_outliers'][key][machine] = list()
                        data_dictionary['unique_outliers'][key][machine] = list()
                    else:
                        data_dictionary['all_outliers'][key][machine] = None
                        data_dictionary['common_outliers'][key][machine] = None
                        data_dictionary['unique_outliers'][key][machine] = None

                if key not in plot_titles:
                    plot_titles[key] = dict()
                if machine not in plot_titles[key]:
                    plot_titles[key][machine] = list()
                if len(data['wallclock_times'][key]) == 0:
                    print('WARNING: Skipping: %s from %s (no executions)' %
                          (key, machine))
                else:
                    benchmark_name = key.split(':')[0]
                    if benchmark_name in BENCHMARKS:
                        benchmark_name = BENCHMARKS[benchmark_name]
                    else:
                        benchmark_name = benchmark_name.title()
                    for p_exec in requested_data[key][machine]:
                        if p_exec >= len(data['wallclock_times'][key]):
                            fatal_error('You requested that process execution %g '
                                'for benchmark %s from machine %s be plotted, but '
                                'the Krun results file for that machine only has '
                                '%g process executions for the benchmark.' %
                                (p_exec, key, machine, len(data['wallclock_times'][key])))
                        # Add run sequence to data dictionary.
                        print 'Adding run sequence to ', key, machine
                        data_dictionary['data'][key][machine].append(data['wallclock_times'][key][p_exec])
                        if not wallclock_only:
                            data_dictionary['cycles_counts'][key][machine].append(data['core_cycle_counts'][key][p_exec])
                            if instr_data:
                                data_dictionary['instr_data'][key][machine].append(
                                    get_instr_data(key, machine, instr_dir, [p_exec])[0])
                            else:
                                data_dictionary['instr_data'][key][machine] = None
                            if changepoints:
                                data_dictionary['changepoints'][key][machine].append(data['changepoints'][key][p_exec])
                                data_dictionary['changepoint_means'][key][machine].append(data['changepoint_means'][key][p_exec])
                            if outliers or unique_outliers:
                                data_dictionary['all_outliers'][key][machine].append(data['all_outliers'][key][p_exec])
                                data_dictionary['common_outliers'][key][machine].append(data['common_outliers'][key][p_exec])
                                data_dictionary['unique_outliers'][key][machine].append(data['unique_outliers'][key][p_exec])

                        # Construct plot title.
                        title = '%s, %s, %s, Proc. exec. #%d' % \
                                (benchmark_name,
                                 key.split(':')[1],
                                 machine_name,
                                 p_exec + 1)
                        plot_titles[key][machine].append(title)

    # Check that every benchmark that was requested has been found in the
    # given Krun results files.
    for key in requested_data:
        for machine in requested_data[key]:
            if (key not in data_dictionary['data'] or
                machine not in data_dictionary['data'][key]):
                fatal_error('You requested that plots for benchmark %s from '
                            'machine %s be produced, but no such data was '
                            'found in the Krun results files.' %
                            (key, machine))

    return data_dictionary, plot_titles


def create_cli_parser():
    """Create a parser to deal with command line switches.
    """
    script = os.path.basename(__file__)
    description = (('Plot data from Krun results file(s).'
                    '\n\nExample usage:\n\t$ python %s results1.json.bz2\n'
                    '\t$ python %s -i --window 250 --median --tukey '
                    '--with-outliers results1.json.bz2 results2.json.bz2\n'
                    '\t$ python %s -b binarytrees:Hotspot:default-java'
                    ' results.json.bz2\n') %
                   (script, script, script))
    parser = argparse.ArgumentParser(description)
    parser.add_argument('json_files',
                        nargs='+',
                        action='append',
                        default=[],
                        type=str,
                        help='One or more Krun result files.')
    parser.add_argument('--outfile', '-o',
                        action='store',
                        dest='outfile',
                        default=None,
                        type=str,
                        help=('Name of the PDF file to write to. If no file is '
                              'specified, charts will be displayed interactively '
                              'and not written to disk.'))
    parser.add_argument('--wallclock-only',
                        action='store_true',
                        dest='wallclock',
                        default=False,
                        help='Only plot wallclock times.')
    parser.add_argument('--benchmark', '-b',
                        action='append',
                        dest='benchmarks',
                        default=[],
                        type=str,
                        help='Only draw charts for specific '
                             'machine:benchmark:vm:variant:pexec quintuplet(s). '
                             'e.g. "-b mc1.example.com:binarytrees:Hotspot:default-java:0. '
                             'will chart the first process execution of the '
                             'binary trees benchmark, in Java, on the Hotspot '
                             'VM, as measured on machine mc1. '
                             'This switch can be used repeatedly to chart '
                             'a number of benchmarks.')
    parser.add_argument('--one-page',
                        action='store_true',
                        dest='one_page',
                        default=False,
                        help='Place all charts on a single page')
    parser.add_argument('--window', '-w',
                        action='store',
                        dest='window_size',
                        default=200,
                        type=int,
                        help='Size of the sliding window used to draw a '
                             'rolling median and/or Tukey interval.')
    parser.add_argument('--median', '-m',
                        action='store_true',
                        dest='median',
                        default=False,
                        help='Draw a rolling median.')
    parser.add_argument('--tukey', '-t',
                        action='store_true',
                        dest='tukey',
                        default=False,
                        help=('Draw an interval around the rolling median '
                              'which indicates the limits of "valid" data '
                              'points (i.e. data not identified as outliers).'))
    parser.add_argument('--xlimits', '-x',
                        action='store',
                        dest='xlimits',
                        default=None,
                        type=str,
                        help="Specify X-axis limits as a comma separated pair "
                             "'start,end'. Samples start from 0. e.g. "
                             "'-x 100,130' will show samples in the range "
                             "100 to 130.")
    parser.add_argument('--inset',
                        action='store_true',
                        dest='inset',
                        default=False,
                        help='Place a small chart plotting a small number of '
                             'values in an inset inside each plot. This is '
                             'intended to make it easier to see detail during '
                             'the warm-up phase of each benchmark.')
    parser.add_argument('--with-outliers',
                        action='store_true',
                        dest='outliers',
                        default=False,
                        help='Annotate outliers. Only use this if your '
                             'Krun results file contains outlier information.')
    parser.add_argument('--with-unique-outliers',
                        action='store_true',
                        dest='unique_outliers',
                        default=False,
                        help='Annotate outliers common to multiple '
                             'executions and those unique to single '
                             'executions differently. Only use this if '
                             'your Krun results file contains outlier '
                             'information.')
    parser.add_argument('--with-changepoint-means',
                        action='store_true',
                        dest='changepoint_means',
                        default=False,
                        help='Annotate changepoints and their means as vertical '
                             'and horizontal lines. Only use this if your Krun '
                             'results file contains changepoint information.')
    parser.add_argument('--with-changepoints',
                        action='store_true',
                        dest='changepoints',
                        default=False,
                        help='Annotate changepoints. Only use this if your Krun '
                             'results file contains changepoint information.')
    parser.add_argument('--no-legend',
                        action='store_true',
                        dest='legend_off',
                        default=True,
                        help='Do not draw a legend.')
    parser.add_argument('--export-size',
                        action='store',
                        default='12, 10',
                        help="Set the total size (comma separated pair, "
                             "in inches) of each process execution's "
                             "plots in the output.")
    parser.add_argument('--cycles-ylimits',
                        action='store',
                        default=None,
                        help="Set the y-axis range for core cycle plots "
                             "(comma separated pair)")
    parser.add_argument('--core-cycles',
                        action='store',
                        dest='core_cycles',
                        default=None,
                        help='Only plot the core cycles for specified cores'
                              '(comma separated). e.g: 0,1,2,3')
    return parser


def fatal_error(msg):
    print('')
    print('FATAL Krun plot error:')
    print('\t'+ msg)
    sys.exit(1)


if __name__ == '__main__':
    parser = create_cli_parser()
    options = parser.parse_args()
    if options.outliers and options.unique_outliers:
        fatal_error('Cannot use --with-outliers and --with-unique-outliers '
                    'together.')
    if options.outfile is None:
        pyplot.switch_backend('TkAgg')
    else:
        from matplotlib.backends.backend_pdf import PdfPages
        print 'Saving results to: %s' % options.outfile
    print('Using matplotlib version %s with backend %s' %
            (matplotlib.__version__, matplotlib.get_backend()))
    print(('Charting with sliding window size: %d, xlimits: %s') %
           (options.window_size, options.xlimits))

    # Set process execution plot-stack sizes
    try:
        sz_x, sz_y = options.export_size.split(',')
        EXPORT_SIZE_INCHES[0] = float(sz_x)
        EXPORT_SIZE_INCHES[1] = float(sz_y)
    except ValueError:
        fatal_error('invalid --export-size argument')

    if options.cycles_ylimits and options.wallclock:
        fatal_error('Cannot use --cycles-ylimits AND --wallclock-only.')

    if options.cycles_ylimits:
        try:
            y1, y2 = options.cycles_ylimits.split(',')
            cycles_ylimits = float(y1), float(y2)
        except ValueError:
            fatal_error('invalid --cycles-ylimits argument')
    else:
        cycles_ylimits = None

    if options.core_cycles and options.wallclock:
        fatal_error('Cannot use --core-cycles AND --wallclock-only.')

    if options.core_cycles and not options.wallclock:
        try:
            cycles_str = options.core_cycles.split(',')
            core_cycles = [int(cycle) for cycle in cycles_str]
        except ValueError:
            fatal_error('invalid --core-cycles argument')
        print 'Plotting cycle counts for core(s): %s' % ','.join([str(core) for core in core_cycles])
    else:
        core_cycles = None

    # Smaller fonts for on-screen plots.
    if options.outfile is None:
        TICK_FONTSIZE = 12
        TITLE_FONT_SIZE = 17
        AXIS_FONTSIZE = 14
        BASE_FONTSIZE = 13

    data, plot_titles = get_data_dictionaries(options.json_files[0],
                            options.benchmarks, options.wallclock,
                            options.outliers, options.unique_outliers,
                            options.changepoints or options.changepoint_means)

    main(options.outfile is None,
         data,
         plot_titles,
         options.window_size,
         options.outfile,
         options.xlimits,
         options.outliers,
         options.unique_outliers,
         options.changepoints,
         options.changepoint_means,
         median=options.median,
         tukey=options.tukey,
         inset=options.inset,
         one_page=options.one_page,
         legend_off=options.legend_off,
         core_cycles=core_cycles,
         cycles_ylimits=cycles_ylimits)
