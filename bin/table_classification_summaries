#!/usr/bin/env python2.7
"""Summarise benchmark classifications.
Must be run after mark_changepoints_in_json.
"""

import argparse
import os
import os.path
import sys

from collections import Counter

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import pretty_print_machine, pretty_print_variant
from warmup.krun_results import read_krun_results_file
from warmup.latex import STYLE_SYMBOLS, preamble, end_document, end_table, escape
from warmup.latex import format_median_error, get_latex_symbol_map, section, start_table
from warmup.statistics import bootstrap_confidence_interval


TITLE = 'Summary of benchmark classifications'
TABLE_FORMAT = 'p{5pt}lcrrr'
TABLE_HEADINGS = """& \\multicolumn{2}{c}{Benchmark} &\\multicolumn{2}{c}{Steady state}&\\multicolumn{1}{c}{Median}\\\\
& \\multicolumn{2}{c}{\\& classification} &\\multicolumn{1}{c}{(iter. \\#)} &\\multicolumn{1}{c}{(secs)}&\\multicolumn{1}{c}{(secs)}\\\\
"""


def main(data_dcts, window_size, latex_file):
    summary_data = dict()
    for machine in data_dcts:
        keys = sorted(data_dcts[machine]['wallclock_times'].keys())
        for key in sorted(keys):
            wallclock_times = data_dcts[machine]['wallclock_times'][key]
            if len(wallclock_times) == 0:
                print ('WARNING: Skipping: %s from %s (no executions)' %
                       (key, machine))
            elif len(wallclock_times[0]) == 0:
                print('WARNING: Skipping: %s from %s (benchmark crashed)' %
                      (key, machine))
            else:
                bench, vm, variant = key.split(':')
                vm_variant = '%s: %s (%s)' % (pretty_print_machine(machine),
                                              vm, pretty_print_variant(variant))
                if vm_variant not in summary_data:
                    summary_data[vm_variant] = dict()
                # Get information for all p_execs of this key.
                categories = list()
                last_segment_means = list()
                last_changepoints = list()
                time_to_steadys = list()
                n_pexecs = len(data_dcts[machine]['wallclock_times'][key])
                for p_exec in xrange(n_pexecs):
                    categories.append(data_dcts[machine]['classifications'][key][p_exec])
                    last_segment_means.append(data_dcts[machine]['changepoint_means'][key][p_exec][-1])
                    # Not all process execs have changepoints. However, all
                    # p_execs will have one or more segment mean.
                    if data_dcts[machine]['changepoints'][key][p_exec]:
                        last_changepoints.append(data_dcts[machine]['changepoints'][key][p_exec][-1])
                        to_steady = 0.0
                        for index in xrange(data_dcts[machine]['changepoints'][key][p_exec][-1]):
                            to_steady += data_dcts[machine]['wallclock_times'][key][p_exec][index]
                        time_to_steadys.append(to_steady)
                    else:  # Flat execution, no changepoints.
                        time_to_steadys.append(0.0)
                # Average all information.
                category, occurences = Counter(categories).most_common()[0]
                if occurences == n_pexecs:
                    reported_category = category
                elif occurences > n_pexecs // 2:
                    reported_category = 'mostly %s' % category
                else:
                    reported_category = 'inconsistent'
                if (reported_category in ('no steady state', 'inconsistent') or
                    reported_category.startswith('mostly ')):
                    mean_last_segment = ''
                    mean_last_cpt = ''
                    time_to_steady = ''
                else:
                    median, error = bootstrap_confidence_interval(last_segment_means)
                    mean_last_segment = format_median_error(median, error)
                    if last_changepoints:
                        median, error = bootstrap_confidence_interval(last_changepoints)
                        mean_last_cpt = format_median_error(median, error, as_integer=True)
                        median_t, error_t = bootstrap_confidence_interval(time_to_steadys)
                        time_to_steady = format_median_error(median_t, error_t, brief=True)
                    else:  # No changepoints in any process executions.
                        mean_last_cpt = format_median_error(median, 0, as_integer=True)
                        time_to_steady = ''
                # Add summary for this benchmark.
                summary_data[vm_variant][bench] = {'style': reported_category,
                    'last_cpt': mean_last_cpt, 'last_mean': mean_last_segment,
                    'time_to_steady_state':time_to_steady}
    # Write out results.
    write_results_as_latex(summary_data, steady_state, latex_file)
    return


def write_results_as_latex(summary, steady_state, tex_file):
    """Write a results file.
    """
    print('Writing data to %s.' % tex_file)
    with open(tex_file, 'w') as fp:
        fp.write(preamble(TITLE))
        fp.write('\\textbf{Note:} steady state expected within the last \\textbf{%d} iterations.'
                 % steady_state)
        fp.write('\n\n')
        fp.write(get_latex_symbol_map())
        for section_heading in sorted(summary):
            fp.write(section(section_heading))
            fp.write(start_table(TABLE_FORMAT, TABLE_HEADINGS))
            for bench in sorted(summary[section_heading]):
                try:
                    # For now we ignore how many process execs were given each
                    # classification and use 'inconsistent' throughout.
                    if summary[section_heading][bench]['style'].startswith('mostly'):
                        classification = STYLE_SYMBOLS['inconsistent']
                    else:
                        classification = STYLE_SYMBOLS[summary[section_heading][bench]['style']]
                except KeyError:
                    print 'Unknown classification style: %s' % summary[section_heading][bench]['style']
                    sys.exit(1)
                fp.write('&\n%s\n&\n%s\n&\n%s\n&\n%s\\\\ \n' % (
                         ' '.join((escape(bench), classification)),
                         summary[section_heading][bench]['last_cpt'],
                         summary[section_heading][bench]['time_to_steady_state'],
                         summary[section_heading][bench]['last_mean']))
            fp.write(end_table())
            fp.write('\\newpage{}')
        fp.write(end_document())
    return


def get_data_dictionaries(json_files):
    """Read a list of BZipped JSON files and return their contents as a
    dictionaries of machine name -> JSON values.
    """
    data_dictionary = dict()
    steady_state = None
    for filename in json_files:
        assert os.path.exists(filename), 'File %s does not exist.' % filename
        print 'Loading: %s' % filename
        data = read_krun_results_file(filename)
        if 'classifications' not in data:
            print 'Please run mark_changepoints_in_json before re-running this script.'
            sys.exit(1)
        machine_name = data['audit']['uname'].split(' ')[1]
        if '.' in machine_name:  # Remove domain, if there is one.
            machine_name = machine_name.split('.')[0]
        if machine_name not in data_dictionary:
            data_dictionary[machine_name] = data
        else:  # We may have two datasets from the same machine.
            for outer_key in data:
                if outer_key == 'audit' or outer_key == 'reboots':
                    continue
                elif outer_key == 'steady_state_expected':
                    assert data_dictionary[machine_name][outer_key] == data[outer_key]
                    continue
                for key in data[outer_key]:
                    assert key not in data_dictionary[machine_name][outer_key]
                    if key not in data_dictionary[machine_name][outer_key]:
                        data_dictionary[machine_name][outer_key][key] = dict()
                    data_dictionary[machine_name][outer_key][key] = data[outer_key][key]
        if steady_state is None:
            steady_state = data['steady_state_expected']
        else:
            assert steady_state == data['steady_state_expected'], \
                   ('Cannot summarise categories generated with different' +
                    ' steady-state-expected values.')
    return steady_state, data_dictionary


def create_cli_parser():
    """Create a parser to deal with command line switches.
    """
    script = os.path.basename(__file__)
    description = (('Summarise benchmark classifications stored within a Krun ' +
                    'results file. Must be run after mark_changepoints_in_json.' +
                    '\n\nExample usage:\n\n' +
                    '\t$ python %s -l summary.tex results.json.bz2') % script)
    parser = argparse.ArgumentParser(description=description,
                                     formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('json_files', action='append', nargs='+', default=[],
                        type=str, help='One or more Krun result files.')
    parser.add_argument('--outfile', '-o', action='store', dest='latex_file',
                        type=str, help=('Name of the LaTeX file to write to.'),
                        required=True)
    return parser


if __name__ == '__main__':
    parser = create_cli_parser()
    options = parser.parse_args()
    steady_state, data_dcts = get_data_dictionaries(options.json_files[0])
    main(data_dcts, steady_state, options.latex_file)
